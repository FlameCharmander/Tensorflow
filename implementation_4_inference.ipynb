{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/seq2seq/imdb/\"\n",
    "source_file = data_path + \"source_id_20.txt\"\n",
    "target_file = data_path + \"target_id_20.txt\"\n",
    "vocab_file = data_path + \"vocab.txt\"\n",
    "data_path = \"data/seq2seq/rumor/\"\n",
    "# source_file = data_path + \"source_id_20.txt\"\n",
    "# target_file = data_path + \"target_id_20.txt\"\n",
    "source_file = data_path + \"source_id.txt\"\n",
    "target_file = data_path + \"target_id.txt\"\n",
    "vocab_file = data_path + \"vocab.txt\"\n",
    "save_path = \"model/save/implementation_4/\"\n",
    "model_file = save_path+\"model-10800.meta\"\n",
    "graph_path = \"./graphs/implementation_4_graph\"\n",
    "EPOCH_NUM = 1000\n",
    "BATCH_SIZE = 64\n",
    "ENCODER_MAX_LENGTH = 3\n",
    "DECODER_MAX_LENGTH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    def __init__(self, batch_size, encoder_max_length, decoder_max_length, vocab_dict):\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder_max_length = encoder_max_length\n",
    "        self.decoder_max_length = decoder_max_length\n",
    "        self.vocab_dict = vocab_dict\n",
    "        \n",
    "    def create_encoder_batches(self, data_file_list):\n",
    "        \"\"\"make self.token_stream into a integer stream.\"\"\"\n",
    "        self.encoder_token_stream = []\n",
    "        print(\"load %s file data..\" % ' '.join(data_file_list))\n",
    "        for data_file in data_file_list:\n",
    "            with open(data_file, 'r', encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    line = line.split()\n",
    "                    parse_line = [int(x) for x in line]\n",
    "                    if len(parse_line) < self.encoder_max_length:\n",
    "                        parse_line.extend([self.vocab_dict['<PAD>']] * (self.encoder_max_length - len(parse_line)))  # padding\n",
    "                    else:\n",
    "                        parse_line = parse_line[:self.encoder_max_length]\n",
    "                    #y.append(self.vocab_dict['<EOS>'])#end token\n",
    "                    self.encoder_token_stream.append(parse_line)\n",
    "        self.encoder_num_batch = int(len(self.encoder_token_stream) / self.batch_size)\n",
    "        # cut the taken_stream's length exactly equal to num_batch * batch_size\n",
    "        self.encoder_token_stream = self.encoder_token_stream[:self.encoder_num_batch * self.batch_size]\n",
    "        self.encoder_sequence_batch = np.split(np.array(self.encoder_token_stream), self.encoder_num_batch, 0)\n",
    "        self.encoder_pointer = 0\n",
    "        print(\"      Load %d * %d encoder batches\" % (self.encoder_num_batch, self.batch_size))\n",
    "\n",
    "    def create_decoder_batches(self, data_file_list):\n",
    "        \"\"\"make self.token_stream into a integer stream.\"\"\"\n",
    "        self.decoder_token_stream = []\n",
    "        print(\"load %s file data..\" % ' '.join(data_file_list))\n",
    "        for data_file in data_file_list:\n",
    "            with open(data_file, 'r', encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    line = line.split()\n",
    "                    parse_line = [int(x) for x in line]\n",
    "                    if len(parse_line) < self.decoder_max_length:\n",
    "                        parse_line.extend([self.vocab_dict['<PAD>']] * (self.decoder_max_length - len(parse_line)))  # padding\n",
    "                    else:\n",
    "                        parse_line = parse_line[:self.decoder_max_length]\n",
    "                    #y.append(self.vocab_dict['<EOS>'])#end token\n",
    "                    self.decoder_token_stream.append(parse_line)\n",
    "        self.decoder_num_batch = int(len(self.decoder_token_stream) / self.batch_size)\n",
    "        # cut the taken_stream's length exactly equal to num_batch * batch_size\n",
    "        self.decoder_token_stream = self.decoder_token_stream[:self.decoder_num_batch * self.batch_size]\n",
    "        self.decoder_sequence_batch = np.split(np.array(self.decoder_token_stream), self.decoder_num_batch, 0)\n",
    "        self.decoder_pointer = 0\n",
    "        print(\"      Load %d * %d decoder batches\" % (self.decoder_num_batch, self.batch_size))    \n",
    "    \n",
    "    def next_encoder_batch(self):\n",
    "        ret = self.encoder_sequence_batch[self.encoder_pointer]\n",
    "        self.encoder_pointer = (self.encoder_pointer + 1) % (self.encoder_num_batch - 5)\n",
    "        #x = np.concatenate([np.tile(self.vocab_dict['<GO>'], self.batch_size), ret], axis=0)\n",
    "        #y = np.concatenate([ret, np.tile(self.vocab_dict['<EOS>'], self.batch_size)], axis=0)\n",
    "        ret = [np.array(x) for x in ret]\n",
    "        return np.array(ret)\n",
    "    \n",
    "    def next_decoder_batch(self):\n",
    "        ret = self.decoder_sequence_batch[self.decoder_pointer]\n",
    "        self.decoder_pointer = (self.decoder_pointer + 1) % (self.decoder_num_batch - 5)\n",
    "        x = np.column_stack((np.tile(self.vocab_dict['<GO>'], self.batch_size), ret))\n",
    "        y = np.column_stack((ret, np.tile(self.vocab_dict['<EOS>'], self.batch_size)))\n",
    "        #x = np.concatenate([np.tile(self.vocab_dict['<GO>'], self.batch_size), ret], axis=0)\n",
    "        #y = np.concatenate([ret, np.tile(self.vocab_dict['<EOS>'], self.batch_size)], axis=0)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emb_data(emb_dict_file):\n",
    "    word_dict = {}\n",
    "    word_list = []\n",
    "    item = 0\n",
    "    with open(emb_dict_file, 'r', encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word = line.strip()\n",
    "            word_dict[word] = item\n",
    "            item += 1\n",
    "            word_list.append(word)\n",
    "    length = len(word_dict)\n",
    "    print(\"Load embedding success! Num: %d\" % length)\n",
    "    return word_dict, length, word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embedding success! Num: 11904\n",
      "load data/seq2seq/rumor/source_id.txt file data..\n",
      "      Load 217 * 64 encoder batches\n",
      "INFO:tensorflow:Restoring parameters from model/save/implementation_4/model-10800\n",
      "刀 砍 李翔 | 曝光 地 <UNK> 李翔 被 <UNK> <UNK>\n",
      "他 给 单子 | 千万 <UNK> <UNK> 他 给 你 的 单子\n",
      "机 尾部 水面 | 东经 海域 被 菲律宾 <UNK> 发现 <UNK> 露出 水面 <UNK> <UNK> 乘客 <UNK> 机组人员 聚集 在 <UNK> 机舱 有 数十\n",
      "事情 真相 水面 | 事情 等 的 事情 <UNK> 大家 <UNK> 有 真相\n",
      "这 是 发生 | 这 是 一 家 <UNK> 被 灭 这 是 真的\n",
      "我 还见过 场面 | 我 要 派 场面 <UNK> <UNK> 过 场面\n",
      "销售 面包 极限 | <UNK> 销售 的 <UNK> 挑战 被 挑战 <UNK> 挑战\n",
      "包括 居然是 化学 | 国内 的 牙膏 包括 儿童 牙膏 <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> 化学 国内 的 牙膏 包括 <UNK> 儿童\n",
      "捐款 不是 让 | 我们 的 捐款 让 我 <UNK> <UNK> 多 的 <UNK>\n",
      "城管 牛逼 反映 | 城管 <UNK> 牛 逼 反映 <UNK> 是 反映\n",
      "宁继春 看 请 | 联系 人 宁继春 <UNK> 看 一 <UNK> 得 转 的 冷漠 的 人 <UNK> 请 你 伸出 手指 <UNK>\n",
      "妇幼 保健院 喝 | <UNK> 妇幼 保健院 提示 您 <UNK> <UNK> 妇幼 保健院 提示 您 请 <UNK> 给 孩子 <UNK> <UNK> 妇幼 保健院 提示\n",
      "政府 还不如 小女孩 | <UNK> 政府 还 说出 傻子 啊\n",
      "政府 必须取缔 狗肉节 | 政府 <UNK> 派 <UNK> 狗肉节\n",
      "大家 一起做 努力 | 大家 一起 为 贞子 的 票房 为 为 贞子 努力\n",
      "举动 拯救 家庭 | 您 的 一个 小 举动 可能 拯救 一个 家庭\n",
      "我们 捐给 红十字会 | <UNK> <UNK> 我们 捐 给 红十字会\n",
      "罪魁祸首 造成 动乱 | 城管 的 恶行 是 造成 <UNK> 的\n",
      "大 事 报道 | <UNK> 大 的 事 没有 一 家 电视新闻 报道 <UNK> 没有\n",
      "我们 有 教学楼 | 我们 有 <UNK> 个 学校 的 教学楼 <UNK> 教学楼 多少 医院 <UNK> <UNK> 这 位 老人 一生 捐赠 <UNK> 数\n",
      "中国人 能怎么办 为了 | 普通 中国 人 还 能 怎么 办 为了 还 为了 还 挺 为了 你 为了 你 为了 这种\n",
      "張 阿姨 錢包 | <UNK> 只是 把 一 分 钱 人民幣 人民幣 当 芙蓉姐姐 把 钱 <UNK> 人民幣 有 元 人民幣\n",
      "媒体 视而不见 不当 | 媒体 网络 <UNK> <UNK> <UNK> <UNK> <UNK> 非法 救助 <UNK>\n",
      "会议 提出 人员 | 月份 内 <UNK> <UNK> 后面 团 的 人员\n",
      "村民 已经坚持 天 | 村民 <UNK> 轮奸 至 其 片\n",
      "人 请 伸出 | <UNK> 看 一 <UNK> 得 转 的 冷漠 的 人 <UNK> 请 你 伸出 手指 <UNK>\n",
      "汪雄 是 抱 | 汪雄 从 监控 上 看 是 被 个 <UNK> 多 岁 男人 <UNK> <UNK> <UNK> <UNK> 急 疯 <UNK>\n",
      "老人 捐赠以 设施 | 这 位 老人 一生 捐赠 <UNK> 数 <UNK> <UNK> 的 医疗 <UNK> 教育\n",
      "气温 近日升高 导 | <UNK> 气温 <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> 工程 <UNK> <UNK> 抗议 小孩 张天翼\n",
      "百姓 延续 风俗 | 因为 这 是 他们 这个 地区 要 长沙贡马乡 红十字 因为 这 是 他们 这个 地区 要 卖 社会 的\n",
      "港口 条件 可比 | 发现 <UNK> 名 <UNK> 港口 条件\n",
      "这场 爆炸从 视频 | 这 好意思 看 <UNK> 对待 爆炸 从 <UNK> <UNK> 看 是 <UNK> 里面 的 对待 <UNK> <UNK> 送\n",
      "外 有 名单 | <UNK> 握有 一 份 位 吸毒 明星 的 名单 外\n",
      "什么 叫 团结 | 高中 <UNK> 上级 看 新闻 <UNK> 看 新闻\n",
      "小女孩 说出 爸爸 | 帮忙 扩散 今天上午 一个 三 岁 多 小 女孩 在 <UNK> <UNK> <UNK> 小区 附近 被 人 拐 走 <UNK> 小\n",
      "一个 是 怀疑 | 一个 <UNK> <UNK> <UNK> 透明 的 面对 <UNK> 的 面对 <UNK> 的 真实性 是\n",
      "释永信 热衷于 事业 | 释永信 <UNK> <UNK> 少林寺 <UNK> <UNK> 事业\n",
      "车 收 粮食 | <UNK> <UNK> 楼下 <UNK> 走 <UNK> 面包车 收 粮食 的 车\n",
      "他 带去 欢乐 | <UNK> 他 给 你 的 生活 带去 <UNK> 无数 的 <UNK> 给 你 一切 你 的 生活\n",
      "中国 制定 罪名 | 中国 <UNK> 要 制定 过 新 <UNK> <UNK> 的 罪名 <UNK>\n",
      "金陵 是 票房 | <UNK> 月日 开 <UNK> 手机 加 <UNK> 是 票房 为 零\n",
      "稳定 是 不会 | 出事 <UNK> <UNK> 出事 <UNK> 是 最后 <UNK> 最后 的 <UNK>\n",
      "朋友圈 是 鉴证者 | 朋友 圈 是 最 大 的 见证 鉴证者\n",
      "那边 跑出 鳄鱼 | 两 <UNK> 那 边 跑 出 到 <UNK> <UNK> 的 鳄鱼\n",
      "中国 儿童 站 | 全国 武术 <UNK> <UNK> 成为 中国 失联 儿童 <UNK>\n",
      "一毛钱 自制驱 蚊水 | 一毛钱 自制 <UNK> 水 <UNK> 无毒\n",
      "肉松 居然是 做 | 肉松 蛋糕 的 肉松 <UNK> 是 棉花 做 的\n",
      "微博 力挺 李天一 | <UNK> <UNK> 挺 <UNK> 影响 因 <UNK> 下午\n",
      "委员 王平 采访 | 全国政协 委员 王平 外国人 王平 外国人 乔布斯 说\n",
      "深圳 走在 前面 | 深圳 <UNK> 次 走 在 <UNK> 全国 <UNK> 的\n",
      "官员 把扔下 楼 | 官员 把 拆迁户 孩子 <UNK> <UNK>\n",
      "香港食环署 正在了解 指出 | 香港 食环署 <UNK> 了解 此 事件 专家 指出\n",
      "转变 会摧垮 观念 | <UNK> 模式 的 转变 会 <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "数学 分数 分 | <UNK> 的 <UNK> 数学 <UNK> 是 <UNK>\n",
      "售卖 是 非法 | 卖 新华字典 是 非法 境外 <UNK>\n",
      "朋友 看见 和尚 | 朋友 在 <UNK> <UNK> <UNK> <UNK> <UNK> 这些 和尚\n",
      "央视 是 对 | 真是 安息 央视 <UNK> 是 <UNK> 尊重\n",
      "他 曾注册 姜成 | 他 <UNK> <UNK> <UNK> 为 父 <UNK> 成 <UNK>\n",
      "中国政府 到底想 隐瞒 | 中国政府 <UNK> <UNK> 没有 什么 新闻 <UNK> 没有 死亡 <UNK> 中国政府 什么 <UNK> 值得 值得 <UNK> 值得 值得 <UNK> 值得\n",
      "附近 拐 女孩 | 实验 小学 寻人启事 今天上午 一个 三 岁 多 小 女孩 在 <UNK> <UNK> 小区 附近 被 人 拐\n",
      "我 也转发 消息 | <UNK> 我 的 消息 <UNK> 该条 消息\n",
      "这 并不是 状态 | 这 <UNK> 是 <UNK> <UNK> <UNK> <UNK> <UNK> 等 <UNK> <UNK> 等 <UNK> <UNK> 的 生活 悲痛欲绝 的\n",
      "我们 有 帮忙 | <UNK> 看到 帮忙 扩散 谢谢 我们 有 <UNK> <UNK>\n",
      "图 为 抱住 | 图 为 从 该 学生 跳 为 警察 为 警察 叔叔\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    vocab_dict, vocab_size, word_list = load_emb_data(vocab_file)\n",
    "    dataloader = Dataloader(BATCH_SIZE, ENCODER_MAX_LENGTH, DECODER_MAX_LENGTH,vocab_dict)\n",
    "    dataloader.create_encoder_batches([source_file])\n",
    "    with tf.Session() as sess:\n",
    "        #restore first\n",
    "        saver = tf.train.import_meta_graph(model_file)\n",
    "        saver.restore(sess,tf.train.latest_checkpoint(save_path))\n",
    "        graph = tf.get_default_graph()\n",
    "        encoder_inputs_placeholder = graph.get_tensor_by_name(\"placeholder/encoder_inputs:0\")\n",
    "        prediction_op = graph.get_tensor_by_name(\"train/prediction:0\")\n",
    "        \n",
    "        for i in range(1):\n",
    "        #for i in range(dataloader.encoder_num_batch):\n",
    "            encoder_inputs = dataloader.next_encoder_batch()\n",
    "            prediction = sess.run([prediction_op], feed_dict={encoder_inputs_placeholder:encoder_inputs})\n",
    "            for source, target in zip(encoder_inputs, prediction[0]):\n",
    "                #print(sentence)\n",
    "                print(\" \".join([word_list[x] for x in source if x != vocab_dict[\"<PAD>\"]]), end=\" | \")\n",
    "                print(\" \".join([word_list[x] for x in target if (x != vocab_dict[\"<PAD>\"] and x != vocab_dict[\"<EOS>\"])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
