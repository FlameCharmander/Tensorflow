{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    生成基于size的，统一分布的数据(size,1)\n",
    "\"\"\"\n",
    "class GenDataLoader():\n",
    "    def __init__(self, size = 200, low = -1, high = 1):\n",
    "        self.size = size\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "\n",
    "    def next_batch(self):\n",
    "        z = np.random.uniform(self.low, self.high, [self.size, 1])\n",
    "        # z = np.linspace(-5.0, 5.0, self.size) + np.random.random(self.size) * 0.01  # sample noise prior\n",
    "        # z = z.reshape([self.size, 1])\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    生成基于mu,sigma,size的正态分布数据(size,1)\n",
    "\"\"\"\n",
    "class RealDataLoader():\n",
    "    def __init__(self, size = 200, mu = -1, sigma = 1):\n",
    "        self.size = size\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def next_batch(self):\n",
    "        data = np.random.normal(self.mu, self.sigma, [self.size ,1])  #(batch_size, size)\n",
    "        data.sort()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP - used for D_pre, D1, D2, G networks\n",
    "def mlp(input, output_dim, is_sigmoid = False):\n",
    "    # construct learnable parameters within local scope\n",
    "    w1=tf.get_variable(\"w0\", [input.get_shape()[1], 6], initializer=tf.random_normal_initializer())\n",
    "    b1=tf.get_variable(\"b0\", [6], initializer=tf.constant_initializer(0.0))\n",
    "    w2=tf.get_variable(\"w1\", [6, 5], initializer=tf.random_normal_initializer())\n",
    "    b2=tf.get_variable(\"b1\", [5], initializer=tf.constant_initializer(0.0))\n",
    "    w3=tf.get_variable(\"w2\", [5,output_dim], initializer=tf.random_normal_initializer())\n",
    "    b3=tf.get_variable(\"b2\", [output_dim], initializer=tf.constant_initializer(0.0))\n",
    "    # nn operators\n",
    "    fc1=tf.nn.tanh(tf.matmul(input,w1)+b1)\n",
    "    fc2=tf.nn.tanh(tf.matmul(fc1,w2)+b2)\n",
    "    if is_sigmoid == False:\n",
    "        fc3=tf.nn.tanh(tf.matmul(fc2,w3)+b3)\n",
    "    else:\n",
    "        fc3=tf.nn.sigmoid(tf.matmul(fc2,w3)+b3)\n",
    "    return fc3, [w1,b1,w2,b2,w3,b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_optimizer(loss,var_list):\n",
    "    batch = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        0.01,                # Base learning rate.\n",
    "        batch,  # Current index into the dataset.\n",
    "        epoch // 4,          # Decay step - this decays 4 times throughout training process.\n",
    "        0.95,                # Decay rate.\n",
    "        staircase=True)\n",
    "    #optimizer=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=batch,var_list=var_list)\n",
    "    optimizer=tf.train.MomentumOptimizer(learning_rate,0.6).minimize(loss,global_step=batch,var_list=var_list)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape, name):\n",
    "    # initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    # return tf.Variable(initial, name=name)\n",
    "    return tf.get_variable(name, shape, initializer=tf.random_normal_initializer())\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    # initial = tf.constant(0.0, shape=shape)\n",
    "    # return tf.Variable(initial, name=name)\n",
    "    return tf.get_variable(name, shape, initializer=tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator():\n",
    "    def __init__(self, inputs, input_size = 1, hidden_size = 6, output_size = 1):\n",
    "        with tf.variable_scope(\"generator\"):\n",
    "            weight1 = weight_variable(shape=[input_size, hidden_size], name=\"weight1\") #(size, 100)\n",
    "            bias1 = bias_variable(shape=[1, hidden_size], name=\"bias1\") #(1, 100)\n",
    "            weight2 = weight_variable(shape=[hidden_size, hidden_size], name=\"weight2\")\n",
    "            bias2 = bias_variable(shape=[1, hidden_size], name=\"bias2\")\n",
    "            weight3 = weight_variable(shape=[hidden_size, output_size], name=\"weight3\")\n",
    "            bias3 = bias_variable(shape=[1, output_size], name=\"bias3\")\n",
    "            frac1 = tf.nn.tanh(tf.matmul(inputs, weight1) + bias1, name=\"frac1\")   #(batch_size, 100)\n",
    "            frac2 = tf.nn.tanh(tf.matmul(frac1, weight2) + bias2, name=\"frac2\")\n",
    "            frac3 = tf.nn.tanh(tf.matmul(frac2, weight3) + bias3, name=\"frac3\")\n",
    "            self.frac = frac3\n",
    "            self.var_list = [weight1, bias1, weight2, bias2, weight3, bias3]\n",
    "            # self.frac, self.var_list = mlp(inputs, 1)\n",
    "            self.frac = tf.multiply(self.frac, 5)\n",
    "    def get_param(self):\n",
    "        return self.frac, self.var_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator():\n",
    "    def __init__(self, inputs, input_size = 1, hidden_size = 6):\n",
    "        with tf.variable_scope(\"discriminator\", reuse=tf.AUTO_REUSE):\n",
    "            weight1 = weight_variable(shape=[input_size, hidden_size], name=\"weight1\") #(size, 100)\n",
    "            bias1 = bias_variable(shape=[1, hidden_size], name=\"bias1\") #(1, 100)\n",
    "            weight2 = weight_variable(shape=[hidden_size, hidden_size], name=\"weight2\")\n",
    "            bias2 = bias_variable(shape=[1, hidden_size], name=\"bias2\")\n",
    "            weight3 = weight_variable(shape=[hidden_size, 1], name=\"weight3\")\n",
    "            bias3 = bias_variable(shape=[1, 1], name=\"bias3\")\n",
    "            frac1 = tf.nn.tanh(tf.matmul(inputs, weight1) + bias1, name=\"frac1\")  # (batch_size, 100)\n",
    "            frac2 = tf.nn.tanh(tf.matmul(frac1, weight2) + bias2, name=\"frac2\") #range()\n",
    "            frac3 = tf.nn.sigmoid(tf.matmul(frac2, weight3) + bias3, name=\"frac3\") #range()\n",
    "            self.frac = frac3\n",
    "            self.var_list = [weight1, bias1, weight2, bias2, weight3, bias3]\n",
    "            # self.frac, self.var_list = mlp(inputs, 1, is_sigmoid = True)\n",
    "\n",
    "    def get_param(self):\n",
    "        return self.frac, self.var_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0  D1: [0.9121418] ,D2: [0.08343625]\n",
      "epoch:50  D1: [0.7986788] ,D2: [0.42630455]\n",
      "epoch:100  D1: [0.45996398] ,D2: [0.4867254]\n",
      "epoch:150  D1: [0.49109086] ,D2: [0.49062195]\n",
      "epoch:200  D1: [0.42364] ,D2: [0.5364584]\n",
      "epoch:250  D1: [0.48003754] ,D2: [0.49085984]\n",
      "epoch:300  D1: [0.53323257] ,D2: [0.48424873]\n",
      "epoch:350  D1: [0.4506283] ,D2: [0.4505711]\n",
      "epoch:400  D1: [0.4262927] ,D2: [0.46688086]\n",
      "epoch:450  D1: [0.4654811] ,D2: [0.44984883]\n",
      "epoch:500  D1: [0.44857147] ,D2: [0.6405948]\n",
      "epoch:550  D1: [0.50686216] ,D2: [0.54887724]\n",
      "epoch:600  D1: [0.48252004] ,D2: [0.498438]\n",
      "epoch:650  D1: [0.4875763] ,D2: [0.46584833]\n",
      "epoch:700  D1: [0.5800265] ,D2: [0.46688703]\n",
      "epoch:750  D1: [0.52935386] ,D2: [0.5269179]\n",
      "epoch:800  D1: [0.4442874] ,D2: [0.56797975]\n",
      "epoch:850  D1: [0.5713089] ,D2: [0.49781248]\n",
      "epoch:900  D1: [0.48709258] ,D2: [0.48379183]\n",
      "epoch:950  D1: [0.5732493] ,D2: [0.61127454]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    size = 200\n",
    "    epoch = 1000    #训练次数\n",
    "    shape = (size, 1)\n",
    "    x_node = tf.placeholder(tf.float32, shape=shape)  # input M normally distributed floats\n",
    "    z_node = tf.placeholder(tf.float32, shape=shape)\n",
    "    generator = Generator(z_node)\n",
    "    G, theta_g = generator.get_param()\n",
    "    discriminator2 = Discriminator(G)\n",
    "    discriminator1 = Discriminator(x_node)\n",
    "    D1, theta_d = discriminator1.get_param()\n",
    "    D2, theta_d = discriminator2.get_param()\n",
    "    loss_d = tf.reduce_mean(tf.log(D1) + tf.log(1 - D2))\n",
    "    loss_g = tf.reduce_mean(tf.log(D2))\n",
    "\n",
    "    # set up optimizer for G,D\n",
    "    train_op_d = momentum_optimizer(1 - loss_d, theta_d)\n",
    "    # train_op_d = tf.train.AdamOptimizer(0.001).minimize(loss =1 - loss_d)\n",
    "    train_op_g = momentum_optimizer(1 - loss_g, theta_g)  # maximize log(D(G(z)))\n",
    "    # train_op_g = tf.train.AdamOptimizer(0.001).minimize(loss=1 - loss_g)  # maximize log(D(G(z)))\n",
    "\n",
    "    sess = tf.InteractiveSession()\n",
    "    tf.global_variables_initializer().run()\n",
    "    gen_data_load = GenDataLoader(size)\n",
    "    real_data_load = RealDataLoader(size)\n",
    "    for i in range(epoch):\n",
    "        for j in range(2):\n",
    "            real_data = real_data_load.next_batch()\n",
    "            gen_data = gen_data_load.next_batch()\n",
    "            sess.run([train_op_d, loss_d], {x_node: real_data, z_node: gen_data})\n",
    "        gen_data = gen_data_load.next_batch()\n",
    "        sess.run([train_op_g, loss_g], {z_node: gen_data})  # update generator\n",
    "        if (i % 50 == 0):\n",
    "            real_data = real_data_load.next_batch()\n",
    "            D1_, D2_ = sess.run([D1, D2], {x_node: real_data, z_node: gen_data})\n",
    "            print(\"epoch:%d \" % i, \"D1:\", D1_[0], \",D2:\", D2_[0])\n",
    "    writer = tf.summary.FileWriter(\"./graphs/implementation_1_graph\", sess.graph)\n",
    "    writer.close()\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
