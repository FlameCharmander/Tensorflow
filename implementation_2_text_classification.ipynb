{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/sentiment_analysis/\"\n",
    "BATCH_SIZE = 48\n",
    "SEQ_LENGTH = 100\n",
    "EMB_DIM = 32 # embedding dimension\n",
    "HIDDEN_DIM = 64 # hidden state dimension of lstm cell\n",
    "emb_dict_file = data_path + \"sst_vocab.txt\"\n",
    "positive_file = data_path + \"sst_pos_sentences_id.txt\"\n",
    "negative_file = data_path + \"sst_neg_sentences_id.txt\"\n",
    "EPOCH_NUM = 1000\n",
    "graph_path = \"./graphs/implementation_2_graph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader():\n",
    "    def __init__(self, batch_size, max_length = 100):\n",
    "        self.batch_size = batch_size\n",
    "        self.sentences = np.array([])\n",
    "        self.labels = np.array([])\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def load_train_data(self, positive_file, negative_file):\n",
    "        # Load data\n",
    "        positive_examples = []\n",
    "        negative_examples = []\n",
    "        with open(positive_file)as fin:\n",
    "            for line in fin:\n",
    "                line = line.strip()\n",
    "                line = line.split()\n",
    "                parse_line = [int(x) for x in line]\n",
    "                parse_line.extend([0] * (self.max_length - len(parse_line)))\n",
    "                if len(parse_line) == self.max_length:\n",
    "                    positive_examples.append(parse_line)\n",
    "        with open(negative_file)as fin:\n",
    "            for line in fin:\n",
    "                line = line.strip()\n",
    "                line = line.split()\n",
    "                parse_line = [int(x) for x in line]\n",
    "                parse_line.extend([0] * (self.max_length - len(parse_line)))\n",
    "                if len(parse_line) == self.max_length:\n",
    "                    negative_examples.append(parse_line)\n",
    "        self.sentences = np.array(positive_examples + negative_examples)\n",
    "\n",
    "        # Generate labels\n",
    "        # positive_labels = [1 for _ in positive_examples]\n",
    "        # negative_labels = [0 for _ in negative_examples]\n",
    "        positive_labels = [[0, 1] for _ in positive_examples]\n",
    "        negative_labels = [[1, 0] for _ in negative_examples]\n",
    "        self.labels = np.concatenate([positive_labels, negative_labels], 0)\n",
    "        # self.labels = positive_labels + negative_labels\n",
    "\n",
    "        # Shuffle the data\n",
    "        shuffle_indices = np.random.permutation(np.arange(len(self.labels)))\n",
    "        self.sentences = self.sentences[shuffle_indices]\n",
    "        self.labels = self.labels[shuffle_indices]\n",
    "\n",
    "        # Split batches\n",
    "        self.num_batch = int(len(self.labels) / self.batch_size)\n",
    "        self.sentences = self.sentences[:self.num_batch * self.batch_size]\n",
    "        self.labels = self.labels[:self.num_batch * self.batch_size]\n",
    "        self.sentences_batches = np.split(self.sentences, self.num_batch, 0)\n",
    "        self.labels_batches = np.split(self.labels, self.num_batch, 0)\n",
    "\n",
    "        self.pointer = 0\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        ret = self.sentences_batches[self.pointer], self.labels_batches[self.pointer]\n",
    "        self.pointer = (self.pointer + 1) % (self.num_batch - 5)\n",
    "        return ret\n",
    "\n",
    "    def test_batch(self):#Preserve part of dataset for testing\n",
    "        ret = self.sentences_batches[self.num_batch - 1], self.labels_batches[self.num_batch - 1]\n",
    "        return ret\n",
    "\n",
    "    def reset_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detection:\n",
    "    def __init__(self, sequence_length, batch_size, vocab_size, emb_dim, hidden_dim = 128, output_keep_prob=0.7):\n",
    "        self.num_emb = vocab_size  # vocab size\n",
    "        self.batch_size = batch_size  # batch size\n",
    "        self.emb_dim = emb_dim  # dimision of embedding\n",
    "        self.hidden_dim = hidden_dim  # hidden size\n",
    "        self.sequence_length = sequence_length  # sequence length\n",
    "        self.output_dim = 2\n",
    "        self.output_keep_prob = output_keep_prob #to prevent overfit\n",
    "        with tf.variable_scope(\"placeholder\"):\n",
    "            self.x = tf.placeholder(shape=[self.batch_size, self.sequence_length], dtype=tf.int32)\n",
    "            self.targets = tf.placeholder(shape=[self.batch_size, self.output_dim], dtype=tf.int64)\n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            self.g_embeddings = tf.Variable(tf.random_uniform([self.num_emb, self.emb_dim], -1.0, 1.0), name=\"W_text\")\n",
    "            self.inputs= tf.nn.embedding_lookup(self.g_embeddings, self.x)  # seq_length x batch_size x emb_dim\n",
    "        with tf.variable_scope(\"rnn\"):\n",
    "            cell_bw = tf.contrib.rnn.BasicLSTMCell(self.hidden_dim, state_is_tuple=False)  # single lstm unit\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, output_keep_prob=self.output_keep_prob)\n",
    "            cell_fw = tf.contrib.rnn.BasicLSTMCell(self.hidden_dim, state_is_tuple=False)  # single lstm unit\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, output_keep_prob=self.output_keep_prob)\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            self.outputs, self.states = tf.nn.bidirectional_dynamic_rnn(cell_bw, cell_fw, self.inputs, dtype=tf.float32)\n",
    "            self.outputs = tf.reshape(self.outputs, shape=[-1, self.sequence_length, self.hidden_dim])\n",
    "            self.outputs = tf.transpose(self.outputs, perm=[1, 0, 2])  # [batch_size,seq_length]\n",
    "            self.outputs = tf.reduce_mean(self.outputs, 0)\n",
    "            self.outputs = self.outputs[:self.batch_size] + self.outputs[self.batch_size:]\n",
    "            self.logits = tf.layers.dense(self.outputs, self.output_dim, name=\"logits\")\n",
    "            self.prob = tf.nn.softmax(self.logits, name=\"softmax_output\")\n",
    "        with tf.variable_scope(\"accuracy\"):\n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.targets, logits=self.logits))\n",
    "            tvars = tf.trainable_variables()\n",
    "            max_grad_norm = 5\n",
    "            # We clip the gradients to prevent explosion\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), max_grad_norm)\n",
    "            gradients = list(zip(grads, tvars))\n",
    "            self.train_op = tf.train.AdamOptimizer(0.005).apply_gradients(gradients)\n",
    "        with tf.variable_scope(\"train\"):\n",
    "            self.accuracy = tf.equal(tf.argmax(self.targets, axis=1), tf.argmax(self.prob, axis=1))\n",
    "            \n",
    "    def train(self, sess, x_batch, y_batch):\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict={self.x:x_batch, self.targets:y_batch})\n",
    "        return loss\n",
    "\n",
    "    def predict(self, sess, x_batch):\n",
    "        prob = sess.run([self.prob], feed_dict={self.x:x_batch})\n",
    "        return prob\n",
    "\n",
    "    def get_accuracy(self, sess, x_batch, y_batch):\n",
    "        accuracy = sess.run([self.accuracy], feed_dict={self.x: x_batch, self.targets: y_batch})\n",
    "        return (accuracy[0].tolist().count(True) / len(x_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emb_data(emb_dict_file):\n",
    "    word_dict = {}\n",
    "    word_list = []\n",
    "    item = 0\n",
    "    with open(emb_dict_file, 'r', encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word = line.strip()\n",
    "            word_dict[word] = item\n",
    "            item += 1\n",
    "            word_list.append(word)\n",
    "    length = len(word_dict)\n",
    "    print(\"Load embedding success! Num: %d\" % length)\n",
    "    return word_dict, length, word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embedding success! Num: 4734\n",
      "WARNING:tensorflow:From <ipython-input-5-a68da2968c2e>:17: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000171F7FF5DD8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x00000171FAC8BDD8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:From <ipython-input-5-a68da2968c2e>:30: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "0, loss:0.696074, accuracy:0.604167\n",
      "20, loss:0.680231, accuracy:0.708333\n",
      "40, loss:0.611574, accuracy:0.812500\n",
      "60, loss:0.337393, accuracy:0.895833\n",
      "80, loss:0.206710, accuracy:0.916667\n",
      "100, loss:0.518600, accuracy:0.916667\n",
      "120, loss:0.252067, accuracy:0.916667\n",
      "140, loss:0.157339, accuracy:0.916667\n",
      "160, loss:0.326981, accuracy:0.916667\n",
      "180, loss:0.315815, accuracy:0.916667\n",
      "200, loss:0.336028, accuracy:0.916667\n",
      "220, loss:0.236249, accuracy:0.895833\n",
      "240, loss:0.381895, accuracy:0.895833\n",
      "260, loss:0.187627, accuracy:0.916667\n",
      "280, loss:0.165750, accuracy:0.916667\n",
      "300, loss:0.074491, accuracy:0.875000\n",
      "320, loss:0.209780, accuracy:0.916667\n",
      "340, loss:0.187793, accuracy:0.916667\n",
      "360, loss:0.231171, accuracy:0.916667\n",
      "380, loss:0.149512, accuracy:0.895833\n",
      "400, loss:0.113589, accuracy:0.895833\n",
      "420, loss:0.127608, accuracy:0.875000\n",
      "440, loss:0.223950, accuracy:0.895833\n",
      "460, loss:0.159105, accuracy:0.916667\n",
      "480, loss:0.119584, accuracy:0.854167\n",
      "500, loss:0.094831, accuracy:0.854167\n",
      "520, loss:0.065820, accuracy:0.854167\n",
      "540, loss:0.145705, accuracy:0.791667\n",
      "560, loss:0.039011, accuracy:0.875000\n",
      "580, loss:0.030108, accuracy:0.895833\n",
      "600, loss:0.047478, accuracy:0.833333\n",
      "620, loss:0.051234, accuracy:0.833333\n",
      "640, loss:0.070101, accuracy:0.812500\n",
      "660, loss:0.059521, accuracy:0.875000\n",
      "680, loss:0.081599, accuracy:0.833333\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    vocab_dict, vocab_size, vocab_list = load_emb_data(emb_dict_file)\n",
    "    dis_data_loader = Dataloader(BATCH_SIZE, SEQ_LENGTH)\n",
    "    dis_data_loader.load_train_data(positive_file, negative_file)\n",
    "    detection = Detection(SEQ_LENGTH, BATCH_SIZE, vocab_size, EMB_DIM, HIDDEN_DIM)\n",
    "    test_x_batch, test_y_batch = dis_data_loader.test_batch()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(graph_path, sess.graph)\n",
    "        writer.close()\n",
    "        for i in range(EPOCH_NUM):\n",
    "            x_batch, y_batch = dis_data_loader.next_batch()\n",
    "            loss = detection.train(sess, x_batch, y_batch)\n",
    "            if (i % 20 == 0):\n",
    "                accuracy = detection.get_accuracy(sess, test_x_batch, test_y_batch)\n",
    "                print(\"%d, loss:%f, accuracy:%f\" % (i, loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
