{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader():\n",
    "    def __init__(self, batch_size, max_length = 100):\n",
    "        self.batch_size = batch_size\n",
    "        self.sentences = np.array([])\n",
    "        self.labels = np.array([])\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def load_train_data(self, positive_file, negative_file):\n",
    "        # Load data\n",
    "        positive_examples = []\n",
    "        negative_examples = []\n",
    "        with open(positive_file)as fin:\n",
    "            for line in fin:\n",
    "                line = line.strip()\n",
    "                line = line.split()\n",
    "                parse_line = [int(x) for x in line]\n",
    "                parse_line.extend([0] * (self.max_length - len(parse_line)))\n",
    "                if len(parse_line) == self.max_length:\n",
    "                    positive_examples.append(parse_line)\n",
    "        with open(negative_file)as fin:\n",
    "            for line in fin:\n",
    "                line = line.strip()\n",
    "                line = line.split()\n",
    "                parse_line = [int(x) for x in line]\n",
    "                parse_line.extend([0] * (self.max_length - len(parse_line)))\n",
    "                if len(parse_line) == self.max_length:\n",
    "                    negative_examples.append(parse_line)\n",
    "        self.sentences = np.array(positive_examples + negative_examples)\n",
    "\n",
    "        # Generate labels\n",
    "        # positive_labels = [1 for _ in positive_examples]\n",
    "        # negative_labels = [0 for _ in negative_examples]\n",
    "        positive_labels = [[0, 1] for _ in positive_examples]\n",
    "        negative_labels = [[1, 0] for _ in negative_examples]\n",
    "        self.labels = np.concatenate([positive_labels, negative_labels], 0)\n",
    "        # self.labels = positive_labels + negative_labels\n",
    "\n",
    "        # Shuffle the data\n",
    "        shuffle_indices = np.random.permutation(np.arange(len(self.labels)))\n",
    "        self.sentences = self.sentences[shuffle_indices]\n",
    "        self.labels = self.labels[shuffle_indices]\n",
    "\n",
    "        # Split batches\n",
    "        self.num_batch = int(len(self.labels) / self.batch_size)\n",
    "        self.sentences = self.sentences[:self.num_batch * self.batch_size]\n",
    "        self.labels = self.labels[:self.num_batch * self.batch_size]\n",
    "        self.sentences_batches = np.split(self.sentences, self.num_batch, 0)\n",
    "        self.labels_batches = np.split(self.labels, self.num_batch, 0)\n",
    "\n",
    "        self.pointer = 0\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        ret = self.sentences_batches[self.pointer], self.labels_batches[self.pointer]\n",
    "        self.pointer = (self.pointer + 1) % (self.num_batch - 5)\n",
    "        return ret\n",
    "\n",
    "    def test_batch(self):#Preserve part of dataset for testing\n",
    "        ret = self.sentences_batches[self.num_batch - 1], self.labels_batches[self.num_batch - 1]\n",
    "        return ret\n",
    "\n",
    "    def reset_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detection:\n",
    "    def __init__(self, sequence_length, batch_size, vocab_size, emb_dim, hidden_dim = 128):\n",
    "        self.num_emb = vocab_size  # vocab size\n",
    "        self.batch_size = batch_size  # batch size\n",
    "        self.emb_dim = emb_dim  # dimision of embedding\n",
    "        self.hidden_dim = hidden_dim  # hidden size\n",
    "        self.sequence_length = sequence_length  # sequence length\n",
    "        self.output_dim = 2\n",
    "        with tf.variable_scope('embedding'):\n",
    "            self.g_embeddings = tf.Variable(tf.random_uniform([self.num_emb, self.emb_dim], -1.0, 1.0), name=\"W_text\")\n",
    "        self.x = tf.placeholder(shape=[self.batch_size, self.sequence_length], dtype=tf.int32)\n",
    "        self.inputs= tf.nn.embedding_lookup(self.g_embeddings, self.x)  # seq_length x batch_size x emb_dim\n",
    "        self.targets = tf.placeholder(shape=[self.batch_size, self.output_dim], dtype=tf.int64)\n",
    "        self.output_keep_prob = 0.7#to prevent overfit\n",
    "        cell_bw = tf.contrib.rnn.BasicLSTMCell(self.hidden_dim, state_is_tuple=False)  # single lstm unit\n",
    "        cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, output_keep_prob=self.output_keep_prob)\n",
    "        cell_fw = tf.contrib.rnn.BasicLSTMCell(self.hidden_dim, state_is_tuple=False)  # single lstm unit\n",
    "        cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, output_keep_prob=self.output_keep_prob)\n",
    "        self.outputs, self.states = tf.nn.bidirectional_dynamic_rnn(cell_bw, cell_fw, self.inputs, dtype=tf.float32)\n",
    "        self.outputs = tf.reshape(self.outputs, shape=[-1, self.sequence_length, self.hidden_dim])\n",
    "        self.outputs = tf.transpose(self.outputs, perm=[1, 0, 2])  # batch_size x seq_length\n",
    "        self.outputs = tf.reduce_mean(self.outputs, 0)\n",
    "        self.outputs = self.outputs[:self.batch_size] + self.outputs[self.batch_size:]\n",
    "        self.logits = tf.layers.dense(self.outputs, self.output_dim, name=\"logits\")\n",
    "        self.prob = tf.nn.softmax(self.logits, name=\"softmax_output\")\n",
    "        \n",
    "        self.accuracy = tf.equal(tf.argmax(self.targets, axis=1), tf.argmax(self.prob, axis=1))\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.targets, logits=self.logits))\n",
    "        tvars = tf.trainable_variables()\n",
    "        max_grad_norm = 5\n",
    "        # We clip the gradients to prevent explosion\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), max_grad_norm)\n",
    "        gradients = list(zip(grads, tvars))\n",
    "        self.train_op = tf.train.AdamOptimizer(0.005).apply_gradients(gradients)\n",
    "        \n",
    "    def train(self, sess, x_batch, y_batch):\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict={self.x:x_batch, self.targets:y_batch})\n",
    "        return loss\n",
    "\n",
    "    def predict(self, sess, x_batch):\n",
    "        prob = sess.run([self.prob], feed_dict={self.x:x_batch})\n",
    "        return prob\n",
    "\n",
    "    def get_accuracy(self, sess, x_batch, y_batch):\n",
    "        accuracy = sess.run([self.accuracy], feed_dict={self.x: x_batch, self.targets: y_batch})\n",
    "        return (accuracy[0].tolist().count(True) / len(x_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/sentiment_analysis/\"\n",
    "BATCH_SIZE = 48\n",
    "SEQ_LENGTH = 100\n",
    "EMB_DIM = 32 # embedding dimension\n",
    "HIDDEN_DIM = 64 # hidden state dimension of lstm cell\n",
    "emb_dict_file = data_path + \"sst_vocab.txt\"\n",
    "positive_file = data_path + \"sst_pos_sentences_id.txt\"\n",
    "negative_file = data_path + \"sst_neg_sentences_id.txt\"\n",
    "EPOCH_NUM = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emb_data(emb_dict_file):\n",
    "    word_dict = {}\n",
    "    word_list = []\n",
    "    item = 0\n",
    "    with open(emb_dict_file, 'r', encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word = line.strip()\n",
    "            word_dict[word] = item\n",
    "            item += 1\n",
    "            word_list.append(word)\n",
    "    length = len(word_dict)\n",
    "    print(\"Load embedding success! Num: %d\" % length)\n",
    "    return word_dict, length, word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embedding success! Num: 4734\n",
      "WARNING:tensorflow:From <ipython-input-3-0be332d4d719>:15: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x0000019921562358>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x000001991EB73C50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "WARNING:tensorflow:From <ipython-input-3-0be332d4d719>:28: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "0, loss:0.681424, accuracy:0.479167\n",
      "20, loss:0.707668, accuracy:0.520833\n",
      "40, loss:0.680380, accuracy:0.625000\n",
      "60, loss:0.548028, accuracy:0.854167\n",
      "80, loss:0.649473, accuracy:0.833333\n",
      "100, loss:0.581155, accuracy:0.875000\n",
      "120, loss:0.248631, accuracy:0.916667\n",
      "140, loss:0.137719, accuracy:0.937500\n",
      "160, loss:0.343888, accuracy:0.937500\n",
      "180, loss:0.343734, accuracy:0.937500\n",
      "200, loss:0.287875, accuracy:0.958333\n",
      "220, loss:0.292525, accuracy:0.958333\n",
      "240, loss:0.259251, accuracy:0.958333\n",
      "260, loss:0.396529, accuracy:0.937500\n",
      "280, loss:0.270605, accuracy:0.937500\n",
      "300, loss:0.205061, accuracy:0.979167\n",
      "320, loss:0.075420, accuracy:0.958333\n",
      "340, loss:0.141421, accuracy:0.958333\n",
      "360, loss:0.190468, accuracy:0.875000\n",
      "380, loss:0.069624, accuracy:0.875000\n",
      "400, loss:0.086599, accuracy:0.958333\n",
      "420, loss:0.087856, accuracy:0.979167\n",
      "440, loss:0.135362, accuracy:0.958333\n",
      "460, loss:0.156351, accuracy:0.916667\n",
      "480, loss:0.098939, accuracy:0.979167\n",
      "500, loss:0.106259, accuracy:0.979167\n",
      "520, loss:0.062990, accuracy:0.875000\n",
      "540, loss:0.167375, accuracy:0.916667\n",
      "560, loss:0.016898, accuracy:0.916667\n",
      "580, loss:0.019641, accuracy:0.958333\n",
      "600, loss:0.053479, accuracy:0.937500\n",
      "620, loss:0.004325, accuracy:0.854167\n",
      "640, loss:0.042347, accuracy:0.895833\n",
      "660, loss:0.058336, accuracy:0.895833\n",
      "680, loss:0.039292, accuracy:0.895833\n",
      "700, loss:0.072189, accuracy:0.916667\n",
      "720, loss:0.021578, accuracy:0.854167\n",
      "740, loss:0.014233, accuracy:0.854167\n",
      "760, loss:0.002815, accuracy:0.854167\n",
      "780, loss:0.011249, accuracy:0.875000\n",
      "800, loss:0.002609, accuracy:0.895833\n",
      "820, loss:0.002677, accuracy:0.875000\n",
      "840, loss:0.003466, accuracy:0.875000\n",
      "860, loss:0.006103, accuracy:0.854167\n",
      "880, loss:0.034614, accuracy:0.875000\n",
      "900, loss:0.002392, accuracy:0.875000\n",
      "920, loss:0.005293, accuracy:0.875000\n",
      "940, loss:0.003767, accuracy:0.895833\n",
      "960, loss:0.005167, accuracy:0.916667\n",
      "980, loss:0.005409, accuracy:0.875000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    vocab_dict, vocab_size, vocab_list = load_emb_data(emb_dict_file)\n",
    "    dis_data_loader = Dataloader(BATCH_SIZE, SEQ_LENGTH)\n",
    "    dis_data_loader.load_train_data(positive_file, negative_file)\n",
    "    detection = Detection(SEQ_LENGTH, BATCH_SIZE, vocab_size, EMB_DIM, HIDDEN_DIM)\n",
    "    test_x_batch, test_y_batch = dis_data_loader.test_batch()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(EPOCH_NUM):\n",
    "            x_batch, y_batch = dis_data_loader.next_batch()\n",
    "            loss = detection.train(sess, x_batch, y_batch)\n",
    "            if (i % 20 == 0):\n",
    "                accuracy = detection.get_accuracy(sess, test_x_batch, test_y_batch)\n",
    "                print(\"%d, loss:%f, accuracy:%f\" % (i, loss, accuracy))\n",
    "        writer = tf.summary.FileWriter(\"graphs/implementation2_graph\", sess.graph)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
