{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emb_data(emb_dict_file):\n",
    "    word_dict = {}\n",
    "    word_list = []\n",
    "    item = 0\n",
    "    with open(emb_dict_file, 'r', encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word = line.strip()\n",
    "            word_dict[word] = item\n",
    "            item += 1\n",
    "            word_list.append(word)\n",
    "    length = len(word_dict)\n",
    "    print(\"Load embedding success! Num: %d\" % length)\n",
    "    return word_dict, length, word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    将集合的记录写入到文件中\n",
    "'''\n",
    "def write_file(filename, line_list, encoding=\"utf-8\"):\n",
    "    with open(filename, \"w\", encoding=encoding) as f:\n",
    "        for line in line_list:\n",
    "            f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(source_filename, target_filename, emb_dict_file):\n",
    "    vocab = set()\n",
    "    stop_notation = [\",\", \".\", \":\", \"\\\"\", \"!\", \":\", \"(\", \")\", \"?\", \";\", \";\", \"[\", \"]\"]\n",
    "    with open(source_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            for word in line.split():\n",
    "                for notation in stop_notation:\n",
    "                    word = word.replace(notation, \"\")\n",
    "                vocab.add(word)\n",
    "                #print(word)\n",
    "        # vocab = list(vocab)\n",
    "    with open(source_filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            for word in line.split():\n",
    "                for notation in stop_notation:\n",
    "                    word = word.replace(notation, \"\")\n",
    "                vocab.add(word)\n",
    "                #print(word)\n",
    "    vocab = list(vocab)\n",
    "    if \"\" in vocab:\n",
    "        vocab.remove(\"\")\n",
    "    vocab.insert(0, \"<GO>\")\n",
    "    vocab.insert(0, \"<EOS>\")\n",
    "    vocab.insert(0, \"<UNK>\")\n",
    "    vocab.insert(0, \"<PAD>\")\n",
    "    write_file(emb_dict_file, vocab)\n",
    "    #print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_id_file(source_file_name, id_file_name, emb_dict_file):\n",
    "    stop_notation = [\",\", \".\", \":\", \"\\\"\", \"!\", \":\", \"(\", \")\", \"?\", \";\", \";\", \"[\", \"]\"]\n",
    "    vocab_dict, vocab_size, vocab_list = load_emb_data(emb_dict_file)\n",
    "    index2word = {value: key for key, value in vocab_dict.items()}\n",
    "    #print(vocab_dict)\n",
    "    #print(index2word)\n",
    "\n",
    "    id_line_list = list()\n",
    "    max_length = 0\n",
    "    with open(source_file_name, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            id_line = list()\n",
    "            length = len(line.split())\n",
    "            if (max_length < length):\n",
    "                max_length = length\n",
    "            for word in line.split(\" \"):\n",
    "                for notation in stop_notation:\n",
    "                    word = word.replace(notation, \"\")\n",
    "                if (word != \"\"):\n",
    "                    id_line.append(str(vocab_dict.get(word, vocab_dict.get(\"<UNK>\"))))\n",
    "                # print(index2word[int(x)], end=\" \")\n",
    "            id_line_list.append(\" \".join(id_line))\n",
    "            # print()\n",
    "    print(max_length)\n",
    "    write_file(id_file_name, id_line_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load embedding success! Num: 11904\n",
      "3\n",
      "Load embedding success! Num: 11904\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#    data_path = \"data/sentiment_analysis/\"\n",
    "#     positive_file = data_path + \"sst_pos_sentences.txt\"\n",
    "#     negative_file = data_path + \"sst_neg_sentences.txt\"\n",
    "#     positive_id_file = data_path + \"sst_pos_sentences_id.txt\"\n",
    "#     negative_id_file = data_path + \"sst_neg_sentences_id.txt\"\n",
    "#     emb_dict_file = data_path + \"sst_vocab.txt\"\n",
    "    data_path = \"data/seq2seq/rumor/\"\n",
    "    positive_file = data_path + \"process_source.txt\"\n",
    "    negative_file = data_path + \"process_target.txt\"\n",
    "    positive_id_file = data_path + \"source_id.txt\"\n",
    "    negative_id_file = data_path + \"target_id.txt\"\n",
    "    emb_dict_file = data_path + \"vocab.txt\"\n",
    "    create_vocab(positive_file, negative_file, emb_dict_file)\n",
    "    create_id_file(positive_file, positive_id_file, emb_dict_file)\n",
    "    create_id_file(negative_file, negative_id_file, emb_dict_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
